const { GoogleGenerativeAI } = require('@google/generative-ai');
const CourseRepository = require('../repositories/courseRepository'); // Importar CourseRepository
const KnowledgeBaseRepository = require('../repositories/knowledgeBaseRepository'); // ‚úÖ 1. Importar el nuevo repositorio
const PythonMLService = require('./pythonMLService'); // ‚úÖ 1. Importar el servicio correcto

// === INICIO: VERIFICACI√ìN DE API KEY ===
const apiKey = process.env.GEMINI_API_KEY;
if (!apiKey) {
    console.error('‚ùå FATAL: La variable de entorno GEMINI_API_KEY no est√° definida.');
    console.error('Aseg√∫rate de que tienes un archivo .env en la ra√≠z del proyecto con "GEMINI_API_KEY=TU_CLAVE_AQUI"');
} else {
    // Muestra solo una parte de la clave por seguridad
    console.log(`‚úÖ GEMINI_API_KEY cargada correctamente (termina en: ...${apiKey.slice(-4)})`);
}
// === FIN: VERIFICACI√ìN DE API KEY ===

const genAI = new GoogleGenerativeAI(apiKey);
const knowledgeBaseRepo = new KnowledgeBaseRepository(); // ‚úÖ 2. Instanciar el repositorio

class MLService {
    /**
     * Procesa un mensaje de usuario usando un Modelo de Lenguaje Grande (LLM).
     * Ya no solo clasifica, sino que genera una respuesta conversacional y extrae la intenci√≥n.
     * @param {string} text - El mensaje del usuario.
     * @param {Array<object>} conversationHistory - El historial de la conversaci√≥n para dar contexto.
     * @returns {Promise<object>} Un objeto con la respuesta, la intenci√≥n y la confianza.
     */
    static async classifyIntent(text, conversationHistory = []) {
        console.log('ü§ñ MLService: Generando respuesta con LLM para:', text);

        // ‚úÖ 3. Cargar la base de conocimiento local al inicio de la funci√≥n.
        const localKB = await knowledgeBaseRepo.load();

        try {
            // Instancia del repositorio de cursos para la herramienta
            const courseRepository = new CourseRepository();

            const model = genAI.getGenerativeModel({
                // Usamos un modelo estable y compatible con herramientas.
                model: "gemini-1.0-pro",
                // ‚úÖ RE-INTRODUCIMOS LAS HERRAMIENTAS
                tools: [{
                    function_declarations: [{
                        name: "getCourseDetails",
                        description: "Obtiene informaci√≥n detallada sobre un curso espec√≠fico de la Universidad Continental, incluyendo su nombre, carrera, temas, materiales (PDFs, videos) y docente. √ötil para responder preguntas sobre cursos, materiales, docentes o temas espec√≠ficos.",
                        parameters: {
                            type: "OBJECT",
                            properties: {
                                courseName: {
                                    type: "STRING",
                                    description: "El nombre completo o parcial del curso a buscar (ej. 'Programaci√≥n I', 'C√°lculo', 'Redes')."
                                }
                            },
                            required: ["courseName"]
                        }
                    }]
                }]
            });
            console.log('ü§ñ MLService: Modelo configurado:', model.model); // Verificaci√≥n del modelo

            // === INICIO: L√ìGICA DE CONVERSACI√ìN CON HERRAMIENTAS ===

            // 1. Instrucci√≥n de sistema: Ahora le decimos que use la herramienta.
            const systemInstruction = {
                role: 'user',
                parts: [{ text: `Eres "Tutor IA UC", un asistente experto de la Universidad Continental. Tu prop√≥sito es ayudar a los estudiantes.
                **Regla Cr√≠tica:** Para obtener informaci√≥n sobre cursos, docentes, horarios o materiales, DEBES usar la herramienta 'getCourseDetails'. No inventes informaci√≥n.
                **Formato de Salida Obligatorio:** Tu respuesta final DEBE ser un √∫nico objeto JSON v√°lido, sin texto adicional.
                El JSON debe tener esta estructura:
                {
                  "intencion": "[consulta_horario|solicitar_material|duda_teorica|consulta_evaluacion|consulta_administrativa|consulta_general]",
                  "confianza": 0.9, // Tu confianza en la clasificaci√≥n de la intenci√≥n
                  "respuesta": "Tu respuesta amable y detallada aqu√≠.",
                  "sugerencias": ["Sugerencia 1", "Sugerencia 2"]
                }`
                }]
            };

            // 2. Formatear el historial y empezar la sesi√≥n de chat.
            const historyForAPI = conversationHistory.map(msg => ({
                role: msg.sender === 'user' ? 'user' : 'model',
                parts: [{ text: msg.text }]
            }));

            const chat = model.startChat({ history: [systemInstruction, ...historyForAPI] });
            let result = await chat.sendMessage(text);

            // 3. Bucle para manejar las llamadas a herramientas.
            while (result.response.functionCalls) {
                const functionCalls = result.response.functionCalls; // CORRECCI√ìN: Es una propiedad, no un m√©todo.
                console.log(`üõ†Ô∏è Gemini solicit√≥ la herramienta: ${functionCalls[0].name}`);

                const call = functionCalls[0];
                if (call.name === 'getCourseDetails') {
                    const courseDetails = await courseRepository.search(call.args.courseName);
                    console.log('üîç Resultado de la herramienta:', courseDetails);

                    // CORRECCI√ìN FINAL: El SDK espera que `response` contenga directamente los datos, no un objeto anidado.
                    result = await chat.sendMessage([{
                        functionResponse: {
                            name: 'getCourseDetails',
                            response: { courseDetails }
                        }
                    }]);
                } else {
                    throw new Error(`Herramienta no reconocida: ${call.name}`);
                }
            }

            // 4. Procesar la respuesta final de Gemini.
            const responseText = result.response.text();
            const jsonStart = responseText.indexOf('{');
            const jsonEnd = responseText.lastIndexOf('}');
            if (jsonStart === -1 || jsonEnd === -1) {
                throw new Error(`La respuesta del LLM no contiene un JSON v√°lido: ${responseText}`);
            }
            const jsonString = responseText.substring(jsonStart, jsonEnd + 1);
            let parsedResult = JSON.parse(jsonString);

            // ‚úÖ 4. Aplicar la capa de validaci√≥n ANTES de devolver el resultado.
            parsedResult = this._validateResponseWithLocalKB(parsedResult, localKB);
            return parsedResult;
            
            // === FIN: L√ìGICA DE CONVERSACI√ìN CON HERRAMIENTAS ===

        } catch (error) {
            console.error('‚ùå Error en MLService al contactar al LLM:', error);
            // Devolver una respuesta de error estandarizada
            return {
                intencion: 'error_general',
                confianza: 1.0,
                respuesta: 'Lo siento, estoy teniendo problemas para conectarme con mi cerebro de IA en este momento. Por favor, intenta de nuevo en unos instantes.'
            };
        }
    }

    /**
     * ‚úÖ NUEVO: Valida la respuesta de la IA contra la base de conocimiento local.
     * @param {object} llmResponse - La respuesta parseada del LLM.
     * @param {Set<string>} localKB - El conjunto de todas las entidades conocidas.
     * @returns {object} La respuesta original o una respuesta de fallback si se detecta una alucinaci√≥n.
     * @private
     */
    static _validateResponseWithLocalKB(llmResponse, localKB) {
        const responseText = llmResponse.respuesta || '';
        
        // Extraer posibles nombres de entidades de la respuesta (texto entre comillas o en may√∫sculas).
        const potentialEntities = (responseText.match(/"'["']/g) || [])
            .map(e => e.replace(/["']/g, '').trim());

        for (const entity of potentialEntities) {
            const normalizedEntity = require('../utils/textUtils').normalizeText(entity);
            if (normalizedEntity.length > 3 && !localKB.has(normalizedEntity)) {
                // ¬°Alucinaci√≥n detectada! La entidad mencionada no existe.
                console.warn(`üö´ ALUCINACI√ìN DETECTADA: La IA mencion√≥ "${entity}", que no existe en la base de conocimiento local.`);
                
                // Devolver una respuesta segura y honesta.
                return {
                    ...llmResponse, // Mantener la intenci√≥n y confianza si es √∫til
                    respuesta: `Mencionaste "${entity}", pero no tengo informaci√≥n sobre eso en mi base de datos. ¬øPodr√≠as reformular tu pregunta o preguntar sobre otro curso o tema?`,
                    sugerencias: ["Ver lista de carreras", "Ver cursos de Ingenier√≠a de Software"]
                };
            }
        }

        // Si todas las entidades son v√°lidas, devolver la respuesta original.
        console.log('‚úÖ Respuesta validada con la base de conocimiento local. Sin alucinaciones.');
        return llmResponse;
    }

    /**
     * Obtiene recomendaciones de cursos y temas relacionados ejecutando scripts de Python.
     * @param {string} query - La consulta de b√∫squeda del usuario.
     * @param {string[]} directResultsIds - IDs de los cursos ya encontrados en la b√∫squeda directa.
     * @returns {Promise<object>} Un objeto con `relatedCourses` y `relatedTopics`.
     */
    static async getRecommendations(query, directResultsIds = []) {
        console.log(`ü§ñ MLService: Obteniendo recomendaciones para "${query}"`);
        // ‚úÖ 2. Delegar la llamada al servicio de Python
        try {
            // Usamos el m√©todo de pythonMLService que ya est√° preparado para llamar al endpoint /recommendations
            const recommendations = await PythonMLService.getRecommendations(query, directResultsIds);
            return recommendations; // El formato ya es { relatedCourses: [...], relatedTopics: [...] }
        } catch (error) {
            console.error('‚ùå Error en MLService al obtener recomendaciones:', error);
            // Devolver un objeto vac√≠o con la estructura esperada en caso de error
            return { relatedCourses: [], relatedTopics: [] };
        }
    }

    // ‚úÖ 3. El m√©todo _runPythonPredictor ya no es necesario y puede ser eliminado.

    /**
     * Genera una descripci√≥n concisa y acad√©mica para un curso espec√≠fico.
     * @param {string} courseName - El nombre del curso.
     * @returns {Promise<string>} La descripci√≥n generada.
     */
    static async generateCourseDescription(courseName) {
        console.log(`ü§ñ MLService: Generando descripci√≥n para el curso: "${courseName}"`);
        try {
            const model = genAI.getGenerativeModel({ model: "gemini-pro" });
            const prompt = `Como un experto acad√©mico y redactor de planes de estudio, crea una descripci√≥n atractiva y concisa (aproximadamente 3 a 4 frases) para el curso universitario llamado "${courseName}". La descripci√≥n debe explicar de qu√© trata el curso, sus objetivos principales y qu√© aprender√°n los estudiantes. El tono debe ser profesional pero accesible.`;

            const result = await model.generateContent(prompt);
            const response = await result.response;
            const description = response.text();

            if (!description) {
                throw new Error("La respuesta de la IA estaba vac√≠a.");
            }

            console.log(`‚úÖ Descripci√≥n de curso generada para "${courseName}"`);
            return description;
        } catch (error) {
            console.error(`‚ùå Error en MLService al generar descripci√≥n para el curso "${courseName}":`, error);
            return "No se pudo generar una descripci√≥n en este momento. Int√©ntalo de nuevo m√°s tarde.";
        }
    }

    /**
     * Genera una descripci√≥n concisa y acad√©mica para un tema espec√≠fico.
     * @param {string} topicName - El nombre del tema.
     * @returns {Promise<string>} La descripci√≥n generada.
     */
    static async generateTopicDescription(topicName) {
        console.log(`ü§ñ MLService: Generando descripci√≥n para el tema: "${topicName}"`);
        try {
            const model = genAI.getGenerativeModel({ model: "gemini-pro" });
            const prompt = `Como un experto acad√©mico, explica brevemente (en 2 o 3 frases) de qu√© trata el tema "${topicName}" en un contexto universitario. S√© claro y conciso.`;

            const result = await model.generateContent(prompt);
            const response = await result.response;
            const description = response.text();
            
            if (!description) {
                throw new Error("La respuesta de la IA estaba vac√≠a.");
            }

            console.log(`‚úÖ Descripci√≥n generada para "${topicName}"`);
            return description;

        } catch (error) {
            console.error(`‚ùå Error en MLService al generar descripci√≥n para "${topicName}":`, error);
            // Devolver un mensaje de error gen√©rico si la IA falla
            return "No se pudo generar una descripci√≥n en este momento. Int√©ntalo de nuevo m√°s tarde.";
        }
    }

    // El m√©todo trainModel ya no es necesario, puedes eliminarlo.
    static async trainModel() {
        console.warn('‚ö†Ô∏è El entrenamiento del modelo local ya no es necesario con la nueva arquitectura LLM.');
        return Promise.resolve({
            success: true,
            message: 'El entrenamiento del modelo local est√° obsoleto. El sistema ahora usa un LLM externo.'
        });
    }
}

module.exports = MLService;