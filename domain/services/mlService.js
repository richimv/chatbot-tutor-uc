const { GoogleGenerativeAI } = require('@google/generative-ai');
const CourseRepository = require('../repositories/courseRepository'); // Importar CourseRepository
const path = require('path');
const { spawn } = require('child_process');
const fs = require('fs').promises;

// === INICIO: VERIFICACI√ìN DE API KEY ===
const apiKey = process.env.GEMINI_API_KEY;
if (!apiKey) {
    console.error('‚ùå FATAL: La variable de entorno GEMINI_API_KEY no est√° definida.');
    console.error('Aseg√∫rate de que tienes un archivo .env en la ra√≠z del proyecto con "GEMINI_API_KEY=TU_CLAVE_AQUI"');
} else {
    // Muestra solo una parte de la clave por seguridad
    console.log(`‚úÖ GEMINI_API_KEY cargada correctamente (termina en: ...${apiKey.slice(-4)})`);
}
// === FIN: VERIFICACI√ìN DE API KEY ===

const genAI = new GoogleGenerativeAI(apiKey);

// --- Constantes para los scripts de Python ---
const PYTHON_PATH = process.env.PYTHON_PATH || 'python';
const PREDICTORS_PATH = path.join(__dirname, '..', '..', 'ml_service', 'predictors');

class MLService {
    /**
     * Procesa un mensaje de usuario usando un Modelo de Lenguaje Grande (LLM).
     * Ya no solo clasifica, sino que genera una respuesta conversacional y extrae la intenci√≥n.
     * @param {string} text - El mensaje del usuario.
     * @param {Array<object>} conversationHistory - El historial de la conversaci√≥n para dar contexto.
     * @returns {Promise<object>} Un objeto con la respuesta, la intenci√≥n y la confianza.
     */
    static async classifyIntent(text, conversationHistory = []) {
        console.log('ü§ñ MLService: Generando respuesta con LLM para:', text);

        try {
            // Instancia del repositorio de cursos para la herramienta
            const courseRepository = new CourseRepository();

            const model = genAI.getGenerativeModel({
                // Usamos un modelo estable y compatible con herramientas.
                model: "gemini-1.0-pro",
                // ‚úÖ RE-INTRODUCIMOS LAS HERRAMIENTAS
                tools: [{
                    function_declarations: [{
                        name: "getCourseDetails",
                        description: "Obtiene informaci√≥n detallada sobre un curso espec√≠fico de la Universidad Continental, incluyendo su nombre, carrera, temas, materiales (PDFs, videos) y docente. √ötil para responder preguntas sobre cursos, materiales, docentes o temas espec√≠ficos.",
                        parameters: {
                            type: "OBJECT",
                            properties: {
                                courseName: {
                                    type: "STRING",
                                    description: "El nombre completo o parcial del curso a buscar (ej. 'Programaci√≥n I', 'C√°lculo', 'Redes')."
                                }
                            },
                            required: ["courseName"]
                        }
                    }]
                }]
            });
            console.log('ü§ñ MLService: Modelo configurado:', model.model); // Verificaci√≥n del modelo

            // === INICIO: L√ìGICA DE CONVERSACI√ìN CON HERRAMIENTAS ===

            // 1. Instrucci√≥n de sistema: Ahora le decimos que use la herramienta.
            const systemInstruction = {
                role: 'user',
                parts: [{ text: `Eres "Tutor IA UC", un asistente experto de la Universidad Continental. Tu prop√≥sito es ayudar a los estudiantes.
                **Regla Cr√≠tica:** Para obtener informaci√≥n sobre cursos, docentes, horarios o materiales, DEBES usar la herramienta 'getCourseDetails'. No inventes informaci√≥n.
                **Formato de Salida Obligatorio:** Tu respuesta final DEBE ser un √∫nico objeto JSON v√°lido, sin texto adicional.
                El JSON debe tener esta estructura:
                {
                  "intencion": "[consulta_horario|solicitar_material|duda_teorica|consulta_evaluacion|consulta_administrativa|consulta_general]",
                  "confianza": 0.9, // Tu confianza en la clasificaci√≥n de la intenci√≥n
                  "respuesta": "Tu respuesta amable y detallada aqu√≠.",
                  "sugerencias": ["Sugerencia 1", "Sugerencia 2"]
                }`
                }]
            };

            // 2. Formatear el historial y empezar la sesi√≥n de chat.
            const historyForAPI = conversationHistory.map(msg => ({
                role: msg.sender === 'user' ? 'user' : 'model',
                parts: [{ text: msg.text }]
            }));

            const chat = model.startChat({ history: [systemInstruction, ...historyForAPI] });
            let result = await chat.sendMessage(text);

            // 3. Bucle para manejar las llamadas a herramientas.
            while (result.response.functionCalls) {
                const functionCalls = result.response.functionCalls; // CORRECCI√ìN: Es una propiedad, no un m√©todo.
                console.log(`üõ†Ô∏è Gemini solicit√≥ la herramienta: ${functionCalls[0].name}`);

                const call = functionCalls[0];
                if (call.name === 'getCourseDetails') {
                    const courseDetails = await courseRepository.search(call.args.courseName);
                    console.log('üîç Resultado de la herramienta:', courseDetails);

                    // CORRECCI√ìN FINAL: El SDK espera que `response` contenga directamente los datos, no un objeto anidado.
                    result = await chat.sendMessage([{
                        functionResponse: {
                            name: 'getCourseDetails',
                            response: { courseDetails }
                        }
                    }]);
                } else {
                    throw new Error(`Herramienta no reconocida: ${call.name}`);
                }
            }

            // 4. Procesar la respuesta final de Gemini.
            const responseText = result.response.text();
            const jsonStart = responseText.indexOf('{');
            const jsonEnd = responseText.lastIndexOf('}');
            if (jsonStart === -1 || jsonEnd === -1) {
                throw new Error(`La respuesta del LLM no contiene un JSON v√°lido: ${responseText}`);
            }
            const jsonString = responseText.substring(jsonStart, jsonEnd + 1);
            const parsedResult = JSON.parse(jsonString);
            return parsedResult;
            
            // === FIN: L√ìGICA DE CONVERSACI√ìN CON HERRAMIENTAS ===

        } catch (error) {
            console.error('‚ùå Error en MLService al contactar al LLM:', error);
            // Devolver una respuesta de error estandarizada
            return {
                intencion: 'error_general',
                confianza: 1.0,
                respuesta: 'Lo siento, estoy teniendo problemas para conectarme con mi cerebro de IA en este momento. Por favor, intenta de nuevo en unos instantes.'
            };
        }
    }

    /**
     * Obtiene recomendaciones de cursos y temas relacionados ejecutando scripts de Python.
     * @param {string} query - La consulta de b√∫squeda del usuario.
     * @param {string[]} directResultsIds - IDs de los cursos ya encontrados en la b√∫squeda directa.
     * @returns {Promise<object>} Un objeto con `relatedCourses` y `relatedTopics`.
     */
    static async getRecommendations(query, directResultsIds = []) {
        console.log(`ü§ñ MLService: Obteniendo recomendaciones para "${query}"`);
        try {
            const coursesDataPath = path.join(__dirname, '..', '..', 'infrastructure', 'database', 'courses.json');
            const coursesData = JSON.parse(await fs.readFile(coursesDataPath, 'utf-8'));

            // Ejecutar predictores contextuales en paralelo. Los predictores de popularidad ya no se llaman aqu√≠.
            const [relatedCourses, relatedTopics] = await Promise.all([
                this._runPythonPredictor('related_course_predictor.py', { query, direct_results_ids: directResultsIds, courses: coursesData }),
                this._runPythonPredictor('related_topic_predictor.py', { query, courses: coursesData })
            ]);

            return {
                relatedCourses,
                relatedTopics
                // Ya no se devuelven popularCourses y popularTopics desde aqu√≠.
            };
        } catch (error) {
            console.error('‚ùå Error en MLService al obtener recomendaciones:', error);
            return { relatedCourses: [], relatedTopics: [], popularCourses: [], popularTopics: null }; // Devolver vac√≠o en caso de error
        }
    }

    /**
     * Helper para ejecutar un script de Python y obtener su salida JSON.
     * @param {string} scriptName - Nombre del archivo del predictor.
     * @param {object} data - Datos a enviar al script v√≠a stdin.
     * @returns {Promise<any>} El resultado JSON del script.
     */
    static _runPythonPredictor(scriptName, data) {
        return new Promise((resolve, reject) => {
            // ‚úÖ SOLUCI√ìN: Ejecutar como un m√≥dulo (-m) para resolver los imports relativos de Python.
            // 1. Convertir la ruta del script a un nombre de m√≥dulo (e.g., ml_service.predictors.script_name)
            const moduleName = `ml_service.predictors.${scriptName.replace('.py', '')}`;
            
            // 2. Establecer el directorio de trabajo en la ra√≠z del proyecto para que Python encuentre el paquete 'ml_service'.
            const projectRoot = path.join(__dirname, '..', '..');

            const pythonProcess = spawn(PYTHON_PATH, ['-m', moduleName], {
                cwd: projectRoot, // Directorio de trabajo
                env: {
                    ...process.env,
                    // Asegurarse de que PYTHONPATH incluya la ra√≠z del proyecto si es necesario.
                    PYTHONPATH: projectRoot,
                },
            });

            let result = '';
            let error = '';

            pythonProcess.stdout.on('data', (data) => { result += data.toString(); });
            pythonProcess.stderr.on('data', (data) => { error += data.toString(); });

            pythonProcess.on('close', (code) => {
                if (code !== 0) return reject(new Error(`Script ${scriptName} fall√≥ con c√≥digo ${code}: ${error}`));
                try { resolve(JSON.parse(result)); } catch (e) { reject(new Error(`Error parseando JSON de ${scriptName}: ${e.message}`)); }
            });

            pythonProcess.stdin.write(JSON.stringify(data));
            pythonProcess.stdin.end();
        });
    }

    /**
     * Genera una descripci√≥n concisa y acad√©mica para un curso espec√≠fico.
     * @param {string} courseName - El nombre del curso.
     * @returns {Promise<string>} La descripci√≥n generada.
     */
    static async generateCourseDescription(courseName) {
        console.log(`ü§ñ MLService: Generando descripci√≥n para el curso: "${courseName}"`);
        try {
            const model = genAI.getGenerativeModel({ model: "gemini-pro" });
            const prompt = `Como un experto acad√©mico y redactor de planes de estudio, crea una descripci√≥n atractiva y concisa (aproximadamente 3 a 4 frases) para el curso universitario llamado "${courseName}". La descripci√≥n debe explicar de qu√© trata el curso, sus objetivos principales y qu√© aprender√°n los estudiantes. El tono debe ser profesional pero accesible.`;

            const result = await model.generateContent(prompt);
            const response = await result.response;
            const description = response.text();

            if (!description) {
                throw new Error("La respuesta de la IA estaba vac√≠a.");
            }

            console.log(`‚úÖ Descripci√≥n de curso generada para "${courseName}"`);
            return description;
        } catch (error) {
            console.error(`‚ùå Error en MLService al generar descripci√≥n para el curso "${courseName}":`, error);
            return "No se pudo generar una descripci√≥n en este momento. Int√©ntalo de nuevo m√°s tarde.";
        }
    }

    /**
     * Genera una descripci√≥n concisa y acad√©mica para un tema espec√≠fico.
     * @param {string} topicName - El nombre del tema.
     * @returns {Promise<string>} La descripci√≥n generada.
     */
    static async generateTopicDescription(topicName) {
        console.log(`ü§ñ MLService: Generando descripci√≥n para el tema: "${topicName}"`);
        try {
            const model = genAI.getGenerativeModel({ model: "gemini-pro" });
            const prompt = `Como un experto acad√©mico, explica brevemente (en 2 o 3 frases) de qu√© trata el tema "${topicName}" en un contexto universitario. S√© claro y conciso.`;

            const result = await model.generateContent(prompt);
            const response = await result.response;
            const description = response.text();
            
            if (!description) {
                throw new Error("La respuesta de la IA estaba vac√≠a.");
            }

            console.log(`‚úÖ Descripci√≥n generada para "${topicName}"`);
            return description;

        } catch (error) {
            console.error(`‚ùå Error en MLService al generar descripci√≥n para "${topicName}":`, error);
            // Devolver un mensaje de error gen√©rico si la IA falla
            return "No se pudo generar una descripci√≥n en este momento. Int√©ntalo de nuevo m√°s tarde.";
        }
    }

    // El m√©todo trainModel ya no es necesario, puedes eliminarlo.
    static async trainModel() {
        console.warn('‚ö†Ô∏è El entrenamiento del modelo local ya no es necesario con la nueva arquitectura LLM.');
        return Promise.resolve({
            success: true,
            message: 'El entrenamiento del modelo local est√° obsoleto. El sistema ahora usa un LLM externo.'
        });
    }
}

module.exports = MLService;